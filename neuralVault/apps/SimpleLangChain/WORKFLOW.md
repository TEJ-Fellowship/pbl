# LangChain System Workflow Documentation

## ğŸ”„ Complete System Workflow

This document explains how the simplified LangChain system works, from PDF files to intelligent chat responses.

## ğŸ“‹ Table of Contents

1. [System Overview](#system-overview)
2. [Workflow Phases](#workflow-phases)
3. [Component Details](#component-details)
4. [Data Flow](#data-flow)
5. [RAG Process](#rag-process)
6. [Memory Storage](#memory-storage)
7. [Usage Guide](#usage-guide)

## ğŸ¯ System Overview

The system demonstrates essential LangChain concepts:

- **Document Loading**: PDF files â†’ Text extraction
- **Text Chunking**: Large documents â†’ Manageable pieces
- **Embeddings**: Text â†’ Numerical vectors
- **Vector Storage**: Searchable vector database
- **RAG**: Retrieval-Augmented Generation for intelligent responses

## ğŸ”„ Workflow Phases

### 1. Initialization Phase

```
User runs: npm start
â†“
main.js loads environment variables (.env)
â†“
Checks for GEMINI_API_KEY
â†“
Creates LangChainDemo instance
```

### 2. Document Loading Phase

```
main.js calls loadDocuments()
â†“
PDFLoader.loadFromDirectory("./test/data")
â†“
Scans directory for .pdf files
â†“
For each PDF:
  â”œâ”€â”€ Read file with fs.readFileSync()
  â”œâ”€â”€ Parse with pdf-parse library
  â”œâ”€â”€ Extract text content
  â””â”€â”€ Create LangChain Document object
â†“
Returns array of Document objects
```

### 3. Text Processing Phase

```
main.js calls TextSplitter
â†“
For each Document:
  â”œâ”€â”€ Split text into chunks (1000 chars each)
  â”œâ”€â”€ Add overlap (200 chars) between chunks
  â””â”€â”€ Create multiple Document objects
â†“
Returns array of chunked Document objects
```

### 4. Embedding Generation Phase

```
VectorStore.addDocuments(chunks)
â†“
For each text chunk:
  â”œâ”€â”€ Send to Gemini API (text-embedding-004)
  â”œâ”€â”€ Receive 768-dimensional vector
  â””â”€â”€ Store vector + original text
â†“
Create MemoryVectorStore with all vectors
```

### 5. Chat Interface Phase

```
User asks question
â†“
Chat.ask(question)
â†“
VectorStore.search(question)
  â”œâ”€â”€ Convert question to embedding
  â”œâ”€â”€ Find similar vectors (cosine similarity)
  â””â”€â”€ Return top 3 most relevant chunks
â†“
Combine chunks into context
â†“
Send to Gemini (gemini-1.5-flash) with prompt:
  "Answer based on context: [chunks] Question: [user question]"
â†“
Return AI response to user
```

## ğŸ”§ Component Details

### PDFLoader (pdf-loader.js)

- **Purpose**: Convert PDF files to text
- **Input**: PDF file path
- **Output**: LangChain Document object
- **Process**: Read file â†’ Parse PDF â†’ Extract text â†’ Create Document
- **Key Methods**:
  - `load()`: Load single PDF file
  - `loadFromDirectory()`: Load all PDFs from directory

### TextSplitter (text-splitter.js)

- **Purpose**: Break large documents into manageable chunks
- **Input**: Document objects
- **Output**: Array of smaller Document objects
- **Process**: Split text â†’ Add overlap â†’ Create chunks
- **Configuration**:
  - Chunk size: 1000 characters
  - Overlap: 200 characters
  - Separators: Paragraphs, lines, words, characters

### VectorStore (vector-store.js)

- **Purpose**: Convert text to searchable vectors
- **Input**: Text chunks
- **Output**: Searchable vector database
- **Process**: Generate embeddings â†’ Store vectors â†’ Enable similarity search
- **Key Methods**:
  - `addDocuments()`: Create embeddings and store vectors
  - `search()`: Find similar documents
  - `searchWithScores()`: Find similar documents with similarity scores

### Chat (chat.js)

- **Purpose**: Provide intelligent responses using RAG
- **Input**: User questions
- **Output**: AI-generated answers
- **Process**: Search relevant chunks â†’ Combine context â†’ Generate response
- **Key Methods**:
  - `ask()`: Main method for getting answers

## ğŸ“Š Data Flow

```
PDF Files â†’ Text Extraction â†’ Chunking â†’ Embeddings â†’ Vector Store â†’ Search â†’ Chat
    â†“              â†“            â†“           â†“            â†“          â†“       â†“
test/data/    Raw Text    Text Chunks   Vectors    Memory Store   Similar   AI
   files      content     (1000 chars)  (768 dim)  (RAM)         Chunks   Response
```

### Detailed Data Transformation:

1. **PDF Files** (test/data/\*.pdf)

   - Binary PDF files
   - Multiple pages, images, formatting

2. **Raw Text** (Extracted content)

   - Plain text from PDFs
   - All formatting removed
   - Metadata preserved

3. **Text Chunks** (Processed segments)

   - 1000 character segments
   - 200 character overlap
   - Maintains context between chunks

4. **Vectors** (Numerical representations)

   - 768-dimensional arrays
   - Generated by Gemini embeddings
   - Capture semantic meaning

5. **Vector Store** (Searchable database)

   - In-memory storage
   - Fast similarity search
   - Cosine similarity matching

6. **Search Results** (Relevant chunks)

   - Top 3 most similar chunks
   - Ranked by similarity score
   - Context for AI response

7. **AI Response** (Final answer)
   - Generated by Gemini
   - Based on retrieved context
   - Natural language response

## ğŸ§  RAG (Retrieval-Augmented Generation) Process

### Step 1: RETRIEVAL

```
User Question â†’ Embedding â†’ Similarity Search â†’ Relevant Chunks
```

- Convert user question to embedding
- Compare with stored document embeddings
- Find most similar chunks using cosine similarity
- Return top 3 most relevant chunks

### Step 2: AUGMENTATION

```
Relevant Chunks + User Question â†’ Context + Question
```

- Format retrieved chunks as context
- Combine with user question
- Create structured prompt for AI
- Include source information

### Step 3: GENERATION

```
Context + Question â†’ AI Model â†’ Intelligent Response
```

- Send formatted prompt to Gemini
- AI processes context and question
- Generate relevant, accurate response
- Return natural language answer

## ğŸ’¾ Memory Storage

### Storage Locations:

- **Documents**: Stored in RAM as Document objects
- **Chunks**: Stored in RAM as separate Document objects
- **Embeddings**: Stored in RAM as numerical vectors
- **Vector Store**: Stored in RAM using MemoryVectorStore
- **No persistent storage**: Everything is lost when program ends

### Memory Management:

- **Efficient**: Only stores necessary data
- **Fast**: In-memory operations are quick
- **Temporary**: Data exists only during session
- **Scalable**: Can handle multiple documents

## ğŸš€ Usage Guide

### Setup:

```bash
# Install dependencies
npm install

# Setup environment
npm run setup

# Add your API key to .env file
GEMINI_API_KEY=your_api_key_here

# Add PDF files to test/data/ directory
```

### Running:

```bash
# Start the application
npm start

# Interactive chat
â“ Your question: What is this document about?
ğŸ’¡ Answer: [AI response based on your documents]
```

### File Structure:

```
SimpleLangChain/
â”œâ”€â”€ main.js              # Main application
â”œâ”€â”€ pdf-loader.js        # PDF processing
â”œâ”€â”€ text-splitter.js     # Text chunking
â”œâ”€â”€ vector-store.js      # Embeddings & search
â”œâ”€â”€ chat.js              # RAG chat interface
â”œâ”€â”€ setup.js             # Setup helper
â”œâ”€â”€ test/data/           # PDF files directory
â””â”€â”€ package.json         # Dependencies
```

## ğŸ” Key Concepts Explained

### Embeddings

- **What**: Numerical representations of text
- **Why**: Enable semantic search (meaning-based, not keyword-based)
- **How**: Gemini converts text to 768-dimensional vectors
- **Example**: "cat" and "feline" have similar embeddings

### Vector Similarity

- **Cosine Similarity**: Measures angle between vectors
- **Range**: -1 (opposite) to 1 (identical)
- **Usage**: Find most similar document chunks
- **Benefit**: Understands meaning, not just keywords

### Chunking Strategy

- **Size**: 1000 characters per chunk
- **Overlap**: 200 characters between chunks
- **Purpose**: Maintain context across boundaries
- **Benefit**: Better retrieval and understanding

### RAG Benefits

- **Accuracy**: Answers based on your documents
- **Relevance**: Only uses relevant information
- **Transparency**: Shows source of information
- **Flexibility**: Works with any document type

## ğŸ¯ Why This Architecture Works

1. **Modular Design**: Each component has a single responsibility
2. **Simple Flow**: Linear progression from PDFs to chat
3. **Memory Efficient**: Uses in-memory storage for speed
4. **Scalable**: Easy to add more document types or features
5. **Educational**: Clear separation of concerns for learning

## ğŸ”§ Troubleshooting

### Common Issues:

- **No PDFs found**: Add PDF files to test/data/ directory
- **API key error**: Check GEMINI_API_KEY in .env file
- **Memory issues**: Reduce chunk size or number of documents
- **Slow responses**: Check internet connection for API calls

### Performance Tips:

- **Chunk size**: Smaller chunks = more precise, larger chunks = more context
- **Overlap**: Higher overlap = better context preservation
- **Documents**: Fewer documents = faster processing
- **API calls**: Embeddings generation is the slowest step

This system demonstrates the core concepts of LangChain: document processing, text chunking, embeddings, vector storage, and RAG - all in a simple, understandable way!
